#!/usr/bin/env python3
"""
DeepSpeed Zero3 Offload é…ç½®éªŒè¯æµ‹è¯•

æ­¤è„šæœ¬éªŒè¯DeepSpeed Zero3ç¯å¢ƒæ˜¯å¦è¢«æ­£ç¡®é…ç½®ï¼Œ
åŒ…æ‹¬æƒé‡å’Œæ¢¯åº¦åˆ°å†…å­˜çš„offloadåŠŸèƒ½æµ‹è¯•ã€‚

åŸºäºDeepSpeed Zero3å®˜æ–¹æ–‡æ¡£ï¼š
https://www.deepspeed.ai/tutorials/zero/
"""

import sys
import warnings
import logging
import json
import os
from typing import List, Tuple, Optional, Dict, Any
import traceback

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('deepspeed_zero3_test.log')
    ]
)
logger = logging.getLogger(__name__)

class DeepSpeedZero3Tester:
    """DeepSpeed Zero3ç¯å¢ƒæµ‹è¯•å™¨"""
    
    def __init__(self):
        self.test_results = []
        self.errors = []
        self.zero3_config = None
        
    def log_result(self, test_name: str, success: bool, message: str = "", error: Optional[Exception] = None):
        """è®°å½•æµ‹è¯•ç»“æœ"""
        result = {
            'test': test_name,
            'success': success,
            'message': message,
            'error': str(error) if error else None
        }
        self.test_results.append(result)
        
        if success:
            logger.info(f"âœ… {test_name}: {message}")
        else:
            logger.error(f"âŒ {test_name}: {message}")
            if error:
                logger.error(f"   é”™è¯¯è¯¦æƒ…: {error}")
                self.errors.append(f"{test_name}: {error}")
    
    def test_basic_imports(self) -> bool:
        """æµ‹è¯•åŸºç¡€åŒ…å¯¼å…¥"""
        test_name = "åŸºç¡€åŒ…å¯¼å…¥æµ‹è¯•"
        try:
            import torch
            import numpy as np
            import transformers
            
            self.log_result(test_name, True, 
                          f"PyTorch: {torch.__version__}, "
                          f"Transformers: {transformers.__version__}, "
                          f"NumPy: {np.__version__}")
            return True
        except ImportError as e:
            self.log_result(test_name, False, "åŸºç¡€åŒ…å¯¼å…¥å¤±è´¥", e)
            return False
    
    def test_deepspeed_imports(self) -> bool:
        """æµ‹è¯•DeepSpeedåŒ…å¯¼å…¥"""
        test_name = "DeepSpeedåŒ…å¯¼å…¥æµ‹è¯•"
        try:
            import deepspeed
            from deepspeed import DeepSpeedEngine
            from deepspeed.runtime.zero.partition_parameters import ZeroParamStatus
            
            self.log_result(test_name, True, f"DeepSpeedç‰ˆæœ¬: {deepspeed.__version__}")
            return True
        except ImportError as e:
            self.log_result(test_name, False, "DeepSpeedåŒ…å¯¼å…¥å¤±è´¥", e)
            return False
    
    def test_cuda_availability(self) -> bool:
        """æµ‹è¯•CUDAå¯ç”¨æ€§"""
        test_name = "CUDAå¯ç”¨æ€§æµ‹è¯•"
        try:
            import torch
            
            if torch.cuda.is_available():
                device_count = torch.cuda.device_count()
                current_device = torch.cuda.current_device()
                device_name = torch.cuda.get_device_name(current_device)
                
                # æ£€æŸ¥GPUå†…å­˜
                total_memory = torch.cuda.get_device_properties(current_device).total_memory
                memory_gb = total_memory / (1024**3)
                
                self.log_result(test_name, True, 
                              f"CUDAå¯ç”¨ï¼Œè®¾å¤‡æ•°é‡: {device_count}, "
                              f"å½“å‰è®¾å¤‡: {current_device}, "
                              f"è®¾å¤‡åç§°: {device_name}, "
                              f"GPUå†…å­˜: {memory_gb:.1f}GB")
            else:
                self.log_result(test_name, True, "CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPU")
            
            return True
        except Exception as e:
            self.log_result(test_name, False, "CUDAæ£€æŸ¥å¤±è´¥", e)
            return False
    
    def create_zero3_config(self) -> Dict[str, Any]:
        """åˆ›å»ºZero3é…ç½®"""
        config = {
            "zero_optimization": {
                "stage": 3,
                "offload_optimizer": {
                    "device": "cpu",
                    "pin_memory": True
                },
                "offload_param": {
                    "device": "cpu",
                    "pin_memory": True
                },
                "overlap_comm": True,
                "contiguous_gradients": True,
                "sub_group_size": int(1e9),
                "reduce_bucket_size": int(5e8),
                "stage3_prefetch_bucket_size": int(5e8),
                "stage3_param_persistence_threshold": int(1e6),
                "stage3_max_live_parameters": int(1e9),
                "stage3_max_reuse_distance": int(1e9),
                "stage3_gather_16bit_weights_on_model_save": True
            },
            "zero_force_ds_cpu_optimizer": False,  # å…è®¸ä½¿ç”¨è‡ªå®šä¹‰ä¼˜åŒ–å™¨
            "gradient_accumulation_steps": 1,
            "gradient_clipping": 1.0,
            "steps_per_print": 2000,
            "train_batch_size": 2,  # ä¿®å¤: micro_batch_per_gpu * gradient_accumulation_steps * world_size = 2 * 1 * 1 = 2
            "train_micro_batch_size_per_gpu": 2,
            "wall_clock_breakdown": False
        }
        return config
    
    def test_zero3_config_creation(self) -> bool:
        """æµ‹è¯•Zero3é…ç½®åˆ›å»º"""
        test_name = "Zero3é…ç½®åˆ›å»ºæµ‹è¯•"
        try:
            self.zero3_config = self.create_zero3_config()
            
            # éªŒè¯é…ç½®ç»“æ„
            assert "zero_optimization" in self.zero3_config
            assert self.zero3_config["zero_optimization"]["stage"] == 3
            assert "offload_optimizer" in self.zero3_config["zero_optimization"]
            assert "offload_param" in self.zero3_config["zero_optimization"]
            
            # ä¿å­˜é…ç½®åˆ°æ–‡ä»¶
            config_path = "ds_config_zero3.json"
            with open(config_path, 'w') as f:
                json.dump(self.zero3_config, f, indent=2)
            
            self.log_result(test_name, True, 
                          f"Zero3é…ç½®åˆ›å»ºæˆåŠŸï¼Œå·²ä¿å­˜åˆ° {config_path}")
            return True
        except Exception as e:
            self.log_result(test_name, False, "Zero3é…ç½®åˆ›å»ºå¤±è´¥", e)
            return False
    
    def test_model_initialization(self) -> bool:
        """æµ‹è¯•æ¨¡å‹åˆå§‹åŒ–"""
        test_name = "æ¨¡å‹åˆå§‹åŒ–æµ‹è¯•"
        try:
            import torch
            import torch.nn as nn
            from transformers import GPT2Config, GPT2LMHeadModel
            
            # åˆ›å»ºä¸€ä¸ªè¾ƒå¤§çš„æ¨¡å‹ç”¨äºæµ‹è¯•Zero3
            config = GPT2Config(
                vocab_size=50257,
                n_positions=1024,
                n_embd=768,
                n_layer=12,
                n_head=12
            )
            
            model = GPT2LMHeadModel(config)
            param_count = sum(p.numel() for p in model.parameters())
            param_size_mb = param_count * 4 / (1024 * 1024)  # å‡è®¾float32
            
            self.log_result(test_name, True, 
                          f"GPT2æ¨¡å‹åˆå§‹åŒ–æˆåŠŸï¼Œå‚æ•°æ•°é‡: {param_count:,}, "
                          f"ä¼°è®¡å¤§å°: {param_size_mb:.1f}MB")
            return True
        except Exception as e:
            self.log_result(test_name, False, "æ¨¡å‹åˆå§‹åŒ–å¤±è´¥", e)
            return False
    
    def test_deepspeed_initialization(self) -> bool:
        """æµ‹è¯•DeepSpeedå¼•æ“åˆå§‹åŒ–"""
        test_name = "DeepSpeedå¼•æ“åˆå§‹åŒ–æµ‹è¯•"
        try:
            import torch
            import torch.nn as nn
            import deepspeed
            from transformers import GPT2Config, GPT2LMHeadModel
            from torch.optim import AdamW
            
            # åˆ›å»ºæ¨¡å‹
            config = GPT2Config(
                vocab_size=50257,
                n_positions=512,  # å‡å°æ¨¡å‹ä»¥é€‚åº”æµ‹è¯•
                n_embd=512,
                n_layer=6,
                n_head=8
            )
            model = GPT2LMHeadModel(config)
            
            # åˆ›å»ºä¼˜åŒ–å™¨
            optimizer = AdamW(model.parameters(), lr=1e-4)
            
            # åˆå§‹åŒ–DeepSpeedå¼•æ“
            model_engine, optimizer, _, _ = deepspeed.initialize(
                model=model,
                optimizer=optimizer,
                config=self.zero3_config,
                model_parameters=model.parameters()
            )
            
            # æ£€æŸ¥Zero3ç‰¹æ€§
            zero_stage = model_engine.zero_optimization_stage()
            
            self.log_result(test_name, True, 
                          f"DeepSpeedå¼•æ“åˆå§‹åŒ–æˆåŠŸï¼ŒZeroé˜¶æ®µ: {zero_stage}")
            return True
        except Exception as e:
            self.log_result(test_name, False, "DeepSpeedå¼•æ“åˆå§‹åŒ–å¤±è´¥", e)
            return False
    
    def test_parameter_offloading(self) -> bool:
        """æµ‹è¯•å‚æ•°offloadåŠŸèƒ½"""
        test_name = "å‚æ•°Offloadæµ‹è¯•"
        try:
            import torch
            import deepspeed
            from transformers import GPT2Config, GPT2LMHeadModel
            from torch.optim import AdamW
            import psutil
            import gc
            
            # è®°å½•åˆå§‹å†…å­˜ä½¿ç”¨
            process = psutil.Process()
            initial_memory = process.memory_info().rss / (1024 * 1024)  # MB
            
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                initial_gpu_memory = torch.cuda.memory_allocated() / (1024 * 1024)  # MB
            else:
                initial_gpu_memory = 0
            
            # åˆ›å»ºè¾ƒå¤§çš„æ¨¡å‹
            config = GPT2Config(
                vocab_size=50257,
                n_positions=1024,
                n_embd=768,
                n_layer=8,
                n_head=12
            )
            model = GPT2LMHeadModel(config)
            optimizer = AdamW(model.parameters(), lr=1e-4)
            
            # ä½¿ç”¨Zero3åˆå§‹åŒ–
            model_engine, optimizer, _, _ = deepspeed.initialize(
                model=model,
                optimizer=optimizer,
                config=self.zero3_config,
                model_parameters=model.parameters()
            )
            
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            batch_size = 2
            seq_length = 512
            input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_length))
            
            if torch.cuda.is_available():
                input_ids = input_ids.cuda()
            
            # å‰å‘ä¼ æ’­
            outputs = model_engine(input_ids, labels=input_ids)
            loss = outputs.loss
            
            # åå‘ä¼ æ’­
            model_engine.backward(loss)
            model_engine.step()
            
            # æ£€æŸ¥å†…å­˜ä½¿ç”¨
            final_memory = process.memory_info().rss / (1024 * 1024)  # MB
            memory_increase = final_memory - initial_memory
            
            if torch.cuda.is_available():
                final_gpu_memory = torch.cuda.memory_allocated() / (1024 * 1024)  # MB
                gpu_memory_increase = final_gpu_memory - initial_gpu_memory
                gpu_info = f", GPUå†…å­˜å¢åŠ : {gpu_memory_increase:.1f}MB"
            else:
                gpu_info = ""
            
            self.log_result(test_name, True, 
                          f"å‚æ•°Offloadæµ‹è¯•æˆåŠŸï¼ŒCPUå†…å­˜å¢åŠ : {memory_increase:.1f}MB{gpu_info}")
            return True
        except Exception as e:
            self.log_result(test_name, False, "å‚æ•°Offloadæµ‹è¯•å¤±è´¥", e)
            return False
    
    def test_gradient_offloading(self) -> bool:
        """æµ‹è¯•æ¢¯åº¦offloadåŠŸèƒ½"""
        test_name = "æ¢¯åº¦Offloadæµ‹è¯•"
        try:
            import torch
            import deepspeed
            from transformers import GPT2Config, GPT2LMHeadModel
            from torch.optim import AdamW
            
            # åˆ›å»ºæ¨¡å‹
            config = GPT2Config(
                vocab_size=50257,
                n_positions=512,
                n_embd=512,
                n_layer=4,
                n_head=8
            )
            model = GPT2LMHeadModel(config)
            optimizer = AdamW(model.parameters(), lr=1e-4)
            
            # ä½¿ç”¨Zero3åˆå§‹åŒ–
            model_engine, optimizer, _, _ = deepspeed.initialize(
                model=model,
                optimizer=optimizer,
                config=self.zero3_config,
                model_parameters=model.parameters()
            )
            
            # å¤šæ­¥è®­ç»ƒæµ‹è¯•æ¢¯åº¦ç´¯ç§¯å’Œoffload
            total_loss = 0
            steps = 3
            
            for step in range(steps):
                # åˆ›å»ºæµ‹è¯•æ•°æ®
                batch_size = 2
                seq_length = 256
                input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_length))
                
                if torch.cuda.is_available():
                    input_ids = input_ids.cuda()
                
                # å‰å‘ä¼ æ’­
                outputs = model_engine(input_ids, labels=input_ids)
                loss = outputs.loss
                total_loss += loss.item()
                
                # åå‘ä¼ æ’­
                model_engine.backward(loss)
                
                # æ¯ä¸¤æ­¥æ›´æ–°ä¸€æ¬¡
                if (step + 1) % 2 == 0:
                    model_engine.step()
            
            avg_loss = total_loss / steps
            
            self.log_result(test_name, True, 
                          f"æ¢¯åº¦Offloadæµ‹è¯•æˆåŠŸï¼Œ{steps}æ­¥å¹³å‡æŸå¤±: {avg_loss:.4f}")
            return True
        except Exception as e:
            self.log_result(test_name, False, "æ¢¯åº¦Offloadæµ‹è¯•å¤±è´¥", e)
            return False
    
    def test_model_checkpointing(self) -> bool:
        """æµ‹è¯•æ¨¡å‹æ£€æŸ¥ç‚¹ä¿å­˜å’ŒåŠ è½½"""
        test_name = "æ¨¡å‹æ£€æŸ¥ç‚¹æµ‹è¯•"
        try:
            import torch
            import deepspeed
            from transformers import GPT2Config, GPT2LMHeadModel
            from torch.optim import AdamW
            import os
            import shutil
            
            checkpoint_dir = "./test_checkpoint"
            
            # æ¸…ç†ä¹‹å‰çš„æ£€æŸ¥ç‚¹
            if os.path.exists(checkpoint_dir):
                shutil.rmtree(checkpoint_dir)
            
            # åˆ›å»ºæ¨¡å‹
            config = GPT2Config(
                vocab_size=50257,
                n_positions=512,
                n_embd=256,
                n_layer=2,
                n_head=4
            )
            model = GPT2LMHeadModel(config)
            optimizer = AdamW(model.parameters(), lr=1e-4)
            
            # ä½¿ç”¨Zero3åˆå§‹åŒ–
            model_engine, optimizer, _, _ = deepspeed.initialize(
                model=model,
                optimizer=optimizer,
                config=self.zero3_config,
                model_parameters=model.parameters()
            )
            
            # è®­ç»ƒä¸€æ­¥
            batch_size = 1
            seq_length = 128
            input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_length))
            
            if torch.cuda.is_available():
                input_ids = input_ids.cuda()
            
            outputs = model_engine(input_ids, labels=input_ids)
            loss = outputs.loss
            model_engine.backward(loss)
            model_engine.step()
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            model_engine.save_checkpoint(checkpoint_dir)
            
            # éªŒè¯æ£€æŸ¥ç‚¹æ–‡ä»¶
            checkpoint_files = os.listdir(checkpoint_dir)
            has_checkpoint = any(f.startswith('global_step') for f in checkpoint_files)
            
            # æ¸…ç†
            if os.path.exists(checkpoint_dir):
                shutil.rmtree(checkpoint_dir)
            
            self.log_result(test_name, True, 
                          f"æ¨¡å‹æ£€æŸ¥ç‚¹æµ‹è¯•æˆåŠŸï¼Œæ£€æŸ¥ç‚¹æ–‡ä»¶: {checkpoint_files}")
            return True
        except Exception as e:
            self.log_result(test_name, False, "æ¨¡å‹æ£€æŸ¥ç‚¹æµ‹è¯•å¤±è´¥", e)
            return False
    
    def test_memory_efficiency(self) -> bool:
        """æµ‹è¯•å†…å­˜æ•ˆç‡"""
        test_name = "å†…å­˜æ•ˆç‡æµ‹è¯•"
        try:
            import torch
            import deepspeed
            from transformers import GPT2Config, GPT2LMHeadModel
            from torch.optim import AdamW
            import psutil
            import gc
            
            # è®°å½•åŸºå‡†å†…å­˜
            process = psutil.Process()
            baseline_memory = process.memory_info().rss / (1024 * 1024)  # MB
            
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                baseline_gpu_memory = torch.cuda.memory_allocated() / (1024 * 1024)  # MB
            
            # æµ‹è¯•ä¸åŒæ¨¡å‹å¤§å°çš„å†…å­˜ä½¿ç”¨
            test_configs = [
                {"n_layer": 2, "n_embd": 256, "name": "å°å‹"},
                {"n_layer": 4, "n_embd": 512, "name": "ä¸­å‹"},
                {"n_layer": 6, "n_embd": 768, "name": "å¤§å‹"}
            ]
            
            memory_results = []
            
            for test_config in test_configs:
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
                # åˆ›å»ºæ¨¡å‹é…ç½®
                config = GPT2Config(
                    vocab_size=50257,
                    n_positions=512,
                    n_embd=test_config["n_embd"],
                    n_layer=test_config["n_layer"],
                    n_head=test_config["n_embd"] // 64
                )
                
                model = GPT2LMHeadModel(config)
                param_count = sum(p.numel() for p in model.parameters())
                
                optimizer = AdamW(model.parameters(), lr=1e-4)
                
                # ä½¿ç”¨Zero3åˆå§‹åŒ–
                model_engine, optimizer, _, _ = deepspeed.initialize(
                    model=model,
                    optimizer=optimizer,
                    config=self.zero3_config,
                    model_parameters=model.parameters()
                )
                
                # æµ‹é‡å†…å­˜ä½¿ç”¨
                current_memory = process.memory_info().rss / (1024 * 1024)
                memory_usage = current_memory - baseline_memory
                
                if torch.cuda.is_available():
                    current_gpu_memory = torch.cuda.memory_allocated() / (1024 * 1024)
                    gpu_memory_usage = current_gpu_memory - baseline_gpu_memory
                else:
                    gpu_memory_usage = 0
                
                memory_results.append({
                    "size": test_config["name"],
                    "params": param_count,
                    "cpu_memory": memory_usage,
                    "gpu_memory": gpu_memory_usage
                })
                
                # æ¸…ç†
                del model_engine, optimizer, model
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            # æ ¼å¼åŒ–ç»“æœ
            result_str = "å†…å­˜ä½¿ç”¨æƒ…å†µ: "
            for result in memory_results:
                result_str += f"{result['size']}æ¨¡å‹({result['params']:,}å‚æ•°): CPU {result['cpu_memory']:.1f}MB"
                if torch.cuda.is_available():
                    result_str += f", GPU {result['gpu_memory']:.1f}MB"
                result_str += "; "
            
            self.log_result(test_name, True, result_str)
            return True
        except Exception as e:
            self.log_result(test_name, False, "å†…å­˜æ•ˆç‡æµ‹è¯•å¤±è´¥", e)
            return False
    
    def test_distributed_setup(self) -> bool:
        """æµ‹è¯•åˆ†å¸ƒå¼è®¾ç½®å…¼å®¹æ€§"""
        test_name = "åˆ†å¸ƒå¼è®¾ç½®å…¼å®¹æ€§æµ‹è¯•"
        try:
            import torch
            import os
            
            # æ£€æŸ¥åˆ†å¸ƒå¼ç¯å¢ƒå˜é‡
            dist_vars = {
                'RANK': os.environ.get('RANK', 'not_set'),
                'WORLD_SIZE': os.environ.get('WORLD_SIZE', 'not_set'),
                'LOCAL_RANK': os.environ.get('LOCAL_RANK', 'not_set'),
                'MASTER_ADDR': os.environ.get('MASTER_ADDR', 'not_set'),
                'MASTER_PORT': os.environ.get('MASTER_PORT', 'not_set')
            }
            
            # æ£€æŸ¥åˆ†å¸ƒå¼åç«¯
            backends = []
            if torch.distributed.is_available():
                if torch.distributed.is_nccl_available():
                    backends.append("NCCL")
                if torch.distributed.is_gloo_available():
                    backends.append("Gloo")
                if torch.distributed.is_mpi_available():
                    backends.append("MPI")
            
            self.log_result(test_name, True, 
                          f"åˆ†å¸ƒå¼ç¯å¢ƒå˜é‡: {dist_vars}, "
                          f"å¯ç”¨åç«¯: {backends}")
            return True
        except Exception as e:
            self.log_result(test_name, False, "åˆ†å¸ƒå¼è®¾ç½®æ£€æŸ¥å¤±è´¥", e)
            return False
    
    def run_all_tests(self) -> bool:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹DeepSpeed Zero3 Offloadé…ç½®éªŒè¯æµ‹è¯•...")
        logger.info("=" * 70)
        
        # æµ‹è¯•åˆ—è¡¨
        tests = [
            self.test_basic_imports,
            self.test_deepspeed_imports,
            self.test_cuda_availability,
            self.test_zero3_config_creation,
            self.test_model_initialization,
            self.test_deepspeed_initialization,
            self.test_parameter_offloading,
            self.test_gradient_offloading,
            self.test_model_checkpointing,
            self.test_memory_efficiency,
            self.test_distributed_setup,
        ]
        
        # è¿è¡Œæµ‹è¯•
        total_tests = len(tests)
        passed_tests = 0
        
        for test_func in tests:
            try:
                if test_func():
                    passed_tests += 1
            except Exception as e:
                logger.error(f"æµ‹è¯•æ‰§è¡Œå¼‚å¸¸: {e}")
                traceback.print_exc()
        
        # è¾“å‡ºæ€»ç»“
        logger.info("=" * 70)
        logger.info(f"ğŸ“Š æµ‹è¯•æ€»ç»“: {passed_tests}/{total_tests} æµ‹è¯•é€šè¿‡")
        
        if passed_tests == total_tests:
            logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼DeepSpeed Zero3 Offloadé…ç½®æ­£ç¡®ã€‚")
            return True
        else:
            logger.warning(f"âš ï¸  {total_tests - passed_tests} ä¸ªæµ‹è¯•å¤±è´¥ã€‚")
            logger.info("ğŸ’¡ å¤±è´¥çš„æµ‹è¯•:")
            for error in self.errors:
                logger.info(f"   - {error}")
            return False
    
    def generate_report(self) -> str:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        report = []
        report.append("DeepSpeed Zero3 Offloadé…ç½®éªŒè¯æŠ¥å‘Š")
        report.append("=" * 50)
        report.append("")
        
        for result in self.test_results:
            status = "âœ… é€šè¿‡" if result['success'] else "âŒ å¤±è´¥"
            report.append(f"{status} {result['test']}")
            if result['message']:
                report.append(f"   {result['message']}")
            if result['error']:
                report.append(f"   é”™è¯¯: {result['error']}")
            report.append("")
        
        # æ€»ç»“
        total = len(self.test_results)
        passed = sum(1 for r in self.test_results if r['success'])
        report.append(f"æ€»è®¡: {passed}/{total} æµ‹è¯•é€šè¿‡")
        
        # æ·»åŠ Zero3é…ç½®ä¿¡æ¯
        if self.zero3_config:
            report.append("")
            report.append("ä½¿ç”¨çš„Zero3é…ç½®:")
            report.append("-" * 30)
            report.append(json.dumps(self.zero3_config, indent=2))
        
        return "\n".join(report)


def main():
    """ä¸»å‡½æ•°"""
    print("DeepSpeed Zero3 Offloadé…ç½®éªŒè¯æµ‹è¯•")
    print("æµ‹è¯•æƒé‡å’Œæ¢¯åº¦åˆ°å†…å­˜çš„offloadåŠŸèƒ½")
    print("=" * 70)
    
    # åˆ›å»ºæµ‹è¯•å™¨å®ä¾‹
    tester = DeepSpeedZero3Tester()
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    success = tester.run_all_tests()
    
    # ç”Ÿæˆå¹¶ä¿å­˜æŠ¥å‘Š
    report = tester.generate_report()
    with open('deepspeed_zero3_report.txt', 'w', encoding='utf-8') as f:
        f.write(report)
    
    logger.info("ğŸ“ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ° deepspeed_zero3_report.txt")
    
    # è¿”å›é€‚å½“çš„é€€å‡ºç 
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
