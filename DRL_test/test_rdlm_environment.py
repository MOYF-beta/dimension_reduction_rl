#!/usr/bin/env python3
"""
RDLMç¯å¢ƒé…ç½®éªŒè¯æµ‹è¯•

æ­¤è„šæœ¬éªŒè¯RDLMï¼ˆå¼ºåŒ–å­¦ä¹ é™ç»´ï¼‰ç¯å¢ƒæ˜¯å¦è¢«æ­£ç¡®é…ç½®ï¼Œ
åŒ…æ‹¬TRLã€transformersã€torchç­‰å¿…è¦ä¾èµ–é¡¹çš„å®‰è£…å’Œé…ç½®ã€‚

åŸºäºHugging Face TRLå¿«é€Ÿå…¥é—¨æŒ‡å—ï¼š
https://hugging-face.cn/docs/trl/quickstart
"""

import sys
import warnings
import logging
from typing import List, Tuple, Optional
import traceback

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('rdlm_test.log')
    ]
)
logger = logging.getLogger(__name__)

class RDLMEnvironmentTester:
    """RDLMç¯å¢ƒæµ‹è¯•å™¨"""
    
    def __init__(self):
        self.test_results = []
        self.errors = []
        
    def log_result(self, test_name: str, success: bool, message: str = "", error: Optional[Exception] = None):
        """è®°å½•æµ‹è¯•ç»“æœ"""
        result = {
            'test': test_name,
            'success': success,
            'message': message,
            'error': str(error) if error else None
        }
        self.test_results.append(result)
        
        if success:
            logger.info(f"âœ… {test_name}: {message}")
        else:
            logger.error(f"âŒ {test_name}: {message}")
            if error:
                logger.error(f"   é”™è¯¯è¯¦æƒ…: {error}")
                self.errors.append(f"{test_name}: {error}")
    
    def test_basic_imports(self) -> bool:
        """æµ‹è¯•åŸºç¡€åŒ…å¯¼å…¥"""
        test_name = "åŸºç¡€åŒ…å¯¼å…¥æµ‹è¯•"
        try:
            import torch
            import numpy as np
            import transformers
            
            self.log_result(test_name, True, 
                          f"PyTorch: {torch.__version__}, "
                          f"Transformers: {transformers.__version__}, "
                          f"NumPy: {np.__version__}")
            return True
        except ImportError as e:
            self.log_result(test_name, False, "åŸºç¡€åŒ…å¯¼å…¥å¤±è´¥", e)
            return False
    
    def test_trl_imports(self) -> bool:
        """æµ‹è¯•TRLåŒ…å¯¼å…¥"""
        test_name = "TRLåŒ…å¯¼å…¥æµ‹è¯•"
        try:
            from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer
            import trl
            
            self.log_result(test_name, True, f"TRLç‰ˆæœ¬: {trl.__version__}")
            return True
        except ImportError as e:
            self.log_result(test_name, False, "TRLåŒ…å¯¼å…¥å¤±è´¥", e)
            return False
    
    def test_cuda_availability(self) -> bool:
        """æµ‹è¯•CUDAå¯ç”¨æ€§"""
        test_name = "CUDAå¯ç”¨æ€§æµ‹è¯•"
        try:
            import torch
            
            if torch.cuda.is_available():
                device_count = torch.cuda.device_count()
                current_device = torch.cuda.current_device()
                device_name = torch.cuda.get_device_name(current_device)
                
                self.log_result(test_name, True, 
                              f"CUDAå¯ç”¨ï¼Œè®¾å¤‡æ•°é‡: {device_count}, "
                              f"å½“å‰è®¾å¤‡: {current_device}, "
                              f"è®¾å¤‡åç§°: {device_name}")
            else:
                self.log_result(test_name, True, "CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPU")
            
            return True
        except Exception as e:
            self.log_result(test_name, False, "CUDAæ£€æŸ¥å¤±è´¥", e)
            return False
    
    def test_tokenizer_loading(self) -> bool:
        """æµ‹è¯•åˆ†è¯å™¨åŠ è½½"""
        test_name = "åˆ†è¯å™¨åŠ è½½æµ‹è¯•"
        try:
            from transformers import GPT2Tokenizer
            
            tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
            tokenizer.pad_token = tokenizer.eos_token
            
            # æµ‹è¯•ç¼–ç 
            test_text = "This is a test sentence."
            encoded = tokenizer.encode(test_text, return_tensors="pt")
            
            self.log_result(test_name, True, 
                          f"GPT2åˆ†è¯å™¨åŠ è½½æˆåŠŸï¼Œè¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
            return True
        except Exception as e:
            self.log_result(test_name, False, "åˆ†è¯å™¨åŠ è½½å¤±è´¥", e)
            return False
    
    def test_model_loading(self) -> bool:
        """æµ‹è¯•æ¨¡å‹åŠ è½½"""
        test_name = "æ¨¡å‹åŠ è½½æµ‹è¯•"
        try:
            from trl import AutoModelForCausalLMWithValueHead
            import torch
            
            # åŠ è½½å°å‹æ¨¡å‹è¿›è¡Œæµ‹è¯•
            model = AutoModelForCausalLMWithValueHead.from_pretrained("gpt2")
            
            # æ£€æŸ¥æ¨¡å‹ç»“æ„
            param_count = sum(p.numel() for p in model.parameters())
            
            self.log_result(test_name, True, 
                          f"GPT2æ¨¡å‹åŠ è½½æˆåŠŸï¼Œå‚æ•°æ•°é‡: {param_count:,}")
            return True
        except Exception as e:
            self.log_result(test_name, False, "æ¨¡å‹åŠ è½½å¤±è´¥", e)
            return False
    
    def test_ppo_trainer_initialization(self) -> bool:
        """æµ‹è¯•PPOè®­ç»ƒå™¨ç»„ä»¶åˆå§‹åŒ–"""
        test_name = "PPOè®­ç»ƒå™¨ç»„ä»¶æµ‹è¯•"
        try:
            from trl import AutoModelForCausalLMWithValueHead, PPOConfig
            from transformers import GPT2Tokenizer, AutoModelForSequenceClassification
            import torch
            import datasets
            
            # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
            model = AutoModelForCausalLMWithValueHead.from_pretrained("gpt2")
            ref_model = AutoModelForCausalLMWithValueHead.from_pretrained("gpt2")
            tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
            tokenizer.pad_token = tokenizer.eos_token
            
            # åˆ›å»ºç®€å•çš„å¥–åŠ±æ¨¡å‹å’Œä»·å€¼æ¨¡å‹
            reward_model = AutoModelForSequenceClassification.from_pretrained("gpt2", num_labels=1)
            value_model = AutoModelForSequenceClassification.from_pretrained("gpt2", num_labels=1)
            
            # åˆ›å»ºç®€å•çš„æ•°æ®é›†
            dummy_data = {"input_ids": [[1, 2, 3]], "attention_mask": [[1, 1, 1]]}
            train_dataset = datasets.Dataset.from_dict(dummy_data)
            
            # åˆå§‹åŒ–PPOé…ç½®
            ppo_config = {"mini_batch_size": 1, "batch_size": 1}
            config = PPOConfig(**ppo_config)
            
            # éªŒè¯æ‰€æœ‰ç»„ä»¶éƒ½èƒ½æ­£ç¡®åˆ›å»º
            components = {
                "ä¸»æ¨¡å‹": model,
                "å‚è€ƒæ¨¡å‹": ref_model,
                "å¥–åŠ±æ¨¡å‹": reward_model,
                "ä»·å€¼æ¨¡å‹": value_model,
                "åˆ†è¯å™¨": tokenizer,
                "é…ç½®": config,
                "æ•°æ®é›†": train_dataset
            }
            
            self.log_result(test_name, True, 
                          f"PPOè®­ç»ƒå™¨æ‰€éœ€ç»„ä»¶åˆ›å»ºæˆåŠŸ: {list(components.keys())}")
            return True
        except Exception as e:
            self.log_result(test_name, False, "PPOè®­ç»ƒå™¨ç»„ä»¶åˆ›å»ºå¤±è´¥", e)
            return False
    
    def test_trl_api_compatibility(self) -> bool:
        """æµ‹è¯•TRL APIå…¼å®¹æ€§"""
        test_name = "TRL APIå…¼å®¹æ€§æµ‹è¯•"
        try:
            from trl import AutoModelForCausalLMWithValueHead, PPOConfig
            from transformers import GPT2Tokenizer
            import torch
            
            # æ£€æŸ¥å…³é”®ç±»å’Œæ–¹æ³•çš„å­˜åœ¨
            model = AutoModelForCausalLMWithValueHead.from_pretrained("gpt2")
            tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
            
            # æ£€æŸ¥æ¨¡å‹ç»“æ„
            has_v_head = hasattr(model, 'v_head')
            has_pretrained_model = hasattr(model, 'pretrained_model')
            has_base_model = hasattr(model, 'base_model') or has_pretrained_model
            
            # æ£€æŸ¥PPOConfig
            config = PPOConfig(mini_batch_size=1, batch_size=1)
            has_config_attrs = all(hasattr(config, attr) for attr in ['mini_batch_size', 'batch_size'])
            
            # æ£€æŸ¥ç”ŸæˆåŠŸèƒ½
            test_input = tokenizer("Hello", return_tensors="pt")
            can_generate = hasattr(model, 'generate')
            
            api_features = {
                "ä»·å€¼å¤´": has_v_head,
                "é¢„è®­ç»ƒæ¨¡å‹": has_pretrained_model,
                "åŸºç¡€æ¨¡å‹": has_base_model,
                "PPOé…ç½®": has_config_attrs,
                "ç”ŸæˆåŠŸèƒ½": can_generate
            }
            
            all_features_ok = all(api_features.values())
            
            self.log_result(test_name, all_features_ok, 
                          f"TRL APIç‰¹æ€§æ£€æŸ¥: {api_features}")
            return all_features_ok
        except Exception as e:
            self.log_result(test_name, False, "TRL APIå…¼å®¹æ€§æ£€æŸ¥å¤±è´¥", e)
            return False
    
    def test_complete_pipeline(self) -> bool:
        """æµ‹è¯•TRLç”Ÿæˆç®¡é“"""
        test_name = "TRLç”Ÿæˆç®¡é“æµ‹è¯•"
        try:
            from trl import AutoModelForCausalLMWithValueHead
            from transformers import GPT2Tokenizer
            import torch
            
            # è®¾ç½®æ¨¡å‹å’Œåˆ†è¯å™¨
            model = AutoModelForCausalLMWithValueHead.from_pretrained("gpt2")
            tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
            tokenizer.pad_token = tokenizer.eos_token
            
            # ç¼–ç æŸ¥è¯¢
            query_txt = "This morning I went to the "
            query_tensor = tokenizer.encode(query_txt, return_tensors="pt")
            
            # ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå“åº”
            with torch.no_grad():
                generation_kwargs = {
                    "min_length": -1,
                    "top_k": 0.0,
                    "top_p": 1.0,
                    "do_sample": True,
                    "pad_token_id": tokenizer.eos_token_id,
                    "max_new_tokens": 20,
                }
                
                # ç›´æ¥ä½¿ç”¨æ¨¡å‹ç”Ÿæˆ
                outputs = model.generate(
                    query_tensor,
                    **generation_kwargs,
                    return_dict_in_generate=True,
                    output_scores=True
                )
                
                # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
                generated_ids = outputs.sequences[0][len(query_tensor[0]):]
                response_txt = tokenizer.decode(generated_ids, skip_special_tokens=True)
                
                # æµ‹è¯•ä»·å€¼å‡½æ•°
                if hasattr(model, 'v_head') and hasattr(model, 'pretrained_model'):
                    # è·å–æœ€åä¸€å±‚éšè—çŠ¶æ€
                    with torch.no_grad():
                        model_output = model.pretrained_model(query_tensor, output_hidden_states=True)
                        hidden_states = model_output.hidden_states[-1]  # æœ€åä¸€å±‚
                        value = model.v_head(hidden_states)
                        value_info = f"ä»·å€¼å¼ é‡å½¢çŠ¶: {value.shape}"
                else:
                    value_info = "ä»·å€¼å¤´ä¸å¯ç”¨"
                
            self.log_result(test_name, True, 
                          f"TRLç”Ÿæˆç®¡é“æµ‹è¯•æˆåŠŸã€‚ç”Ÿæˆæ–‡æœ¬: '{response_txt.strip()}', "
                          f"{value_info}")
            return True
        except Exception as e:
            self.log_result(test_name, False, "TRLç”Ÿæˆç®¡é“æµ‹è¯•å¤±è´¥", e)
            return False
    
    def test_environment_variables(self) -> bool:
        """æµ‹è¯•ç¯å¢ƒå˜é‡"""
        test_name = "ç¯å¢ƒå˜é‡æµ‹è¯•"
        try:
            import os
            
            important_vars = ['CUDA_VISIBLE_DEVICES', 'HF_HOME', 'TRANSFORMERS_CACHE']
            found_vars = {}
            
            for var in important_vars:
                value = os.environ.get(var)
                if value:
                    found_vars[var] = value
            
            self.log_result(test_name, True, 
                          f"æ‰¾åˆ°ç¯å¢ƒå˜é‡: {found_vars}" if found_vars else "æœªæ‰¾åˆ°ç‰¹æ®Šç¯å¢ƒå˜é‡")
            return True
        except Exception as e:
            self.log_result(test_name, False, "ç¯å¢ƒå˜é‡æ£€æŸ¥å¤±è´¥", e)
            return False
    
    def test_memory_usage(self) -> bool:
        """æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        test_name = "å†…å­˜ä½¿ç”¨æµ‹è¯•"
        try:
            import psutil
            import torch
            
            # ç³»ç»Ÿå†…å­˜
            memory = psutil.virtual_memory()
            
            # GPUå†…å­˜ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            gpu_info = ""
            if torch.cuda.is_available():
                gpu_memory = torch.cuda.get_device_properties(0).total_memory
                gpu_info = f", GPUå†…å­˜: {gpu_memory / 1024**3:.1f}GB"
            
            self.log_result(test_name, True, 
                          f"ç³»ç»Ÿå†…å­˜: {memory.total / 1024**3:.1f}GB (å¯ç”¨: {memory.available / 1024**3:.1f}GB){gpu_info}")
            return True
        except ImportError:
            self.log_result(test_name, True, "psutilæœªå®‰è£…ï¼Œè·³è¿‡è¯¦ç»†å†…å­˜æ£€æŸ¥")
            return True
        except Exception as e:
            self.log_result(test_name, False, "å†…å­˜æ£€æŸ¥å¤±è´¥", e)
            return False
    
    def run_all_tests(self) -> bool:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹RDLMç¯å¢ƒé…ç½®éªŒè¯æµ‹è¯•...")
        logger.info("=" * 60)
        
        # æµ‹è¯•åˆ—è¡¨
        tests = [
            self.test_basic_imports,
            self.test_trl_imports,
            self.test_cuda_availability,
            self.test_environment_variables,
            self.test_memory_usage,
            self.test_tokenizer_loading,
            self.test_model_loading,
            self.test_trl_api_compatibility,
            self.test_ppo_trainer_initialization,
            self.test_complete_pipeline,
        ]
        
        # è¿è¡Œæµ‹è¯•
        total_tests = len(tests)
        passed_tests = 0
        
        for test_func in tests:
            try:
                if test_func():
                    passed_tests += 1
            except Exception as e:
                logger.error(f"æµ‹è¯•æ‰§è¡Œå¼‚å¸¸: {e}")
                traceback.print_exc()
        
        # è¾“å‡ºæ€»ç»“
        logger.info("=" * 60)
        logger.info(f"ğŸ“Š æµ‹è¯•æ€»ç»“: {passed_tests}/{total_tests} æµ‹è¯•é€šè¿‡")
        
        if passed_tests == total_tests:
            logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼RDLMç¯å¢ƒé…ç½®æ­£ç¡®ã€‚")
            return True
        else:
            logger.warning(f"âš ï¸  {total_tests - passed_tests} ä¸ªæµ‹è¯•å¤±è´¥ã€‚")
            logger.info("ğŸ’¡ å¤±è´¥çš„æµ‹è¯•:")
            for error in self.errors:
                logger.info(f"   - {error}")
            return False
    
    def generate_report(self) -> str:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        report = []
        report.append("RDLMç¯å¢ƒé…ç½®éªŒè¯æŠ¥å‘Š")
        report.append("=" * 40)
        report.append("")
        
        for result in self.test_results:
            status = "âœ… é€šè¿‡" if result['success'] else "âŒ å¤±è´¥"
            report.append(f"{status} {result['test']}")
            if result['message']:
                report.append(f"   {result['message']}")
            if result['error']:
                report.append(f"   é”™è¯¯: {result['error']}")
            report.append("")
        
        # æ€»ç»“
        total = len(self.test_results)
        passed = sum(1 for r in self.test_results if r['success'])
        report.append(f"æ€»è®¡: {passed}/{total} æµ‹è¯•é€šè¿‡")
        
        return "\n".join(report)


def main():
    """ä¸»å‡½æ•°"""
    print("RDLMç¯å¢ƒé…ç½®éªŒè¯æµ‹è¯•")
    print("åŸºäºHugging Face TRLå¿«é€Ÿå…¥é—¨æŒ‡å—")
    print("=" * 60)
    
    # åˆ›å»ºæµ‹è¯•å™¨å®ä¾‹
    tester = RDLMEnvironmentTester()
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    success = tester.run_all_tests()
    
    # ç”Ÿæˆå¹¶ä¿å­˜æŠ¥å‘Š
    report = tester.generate_report()
    with open('rdlm_environment_report.txt', 'w', encoding='utf-8') as f:
        f.write(report)
    
    logger.info("ğŸ“ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ° rdlm_environment_report.txt")
    
    # è¿”å›é€‚å½“çš„é€€å‡ºç 
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
